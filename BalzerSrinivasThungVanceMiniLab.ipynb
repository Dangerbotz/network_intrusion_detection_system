{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: SVM & LR Classification\n",
    "\n",
    "There are two natural dependent variables in our dataset: label and attack_cat\n",
    "Initial approach is to prep the existing variables for classification and throw in the \"kitchen sink\" and baseline our accuracy. Then look further to see what variables my be cross correlated, or less weight and remove them to see if we can increase our accuracy.\n",
    "\n",
    "In this mini-project, we decided upon trying two different LR classifications and two different SVM classifications. The first we tries was a LR and SVM classification on the full dataset, variable \"df\" in our code and referred to as the \"kitchen sink\" model. We then tried doing the LR and SVM classification on the five most correlated features to the label feature, our dependent variable. We'll refer to this one as the \"five-feature\" model, which is represented by the variable \"df_five\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', None)    # set the max columns to show to unlimited\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82332 entries, 0 to 82331\n",
      "Data columns (total 6 columns):\n",
      "sttl                82332 non-null int64\n",
      "ct_dst_sport_ltm    82332 non-null int64\n",
      "ct_src_dport_ltm    82332 non-null int64\n",
      "swin                82332 non-null int64\n",
      "dwin                82332 non-null int64\n",
      "label               82332 non-null int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 3.8 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('UNSW_NB15_training_set.csv', encoding='utf-8-sig')  # specifing encoding to get rid of the UTF- Byte order Mark (BOM) in the id field\n",
    "df_five = pd.read_csv('miniLab.csv') # read in the five features most correlated to the label feature\n",
    "\n",
    "df_five.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82332 entries, 0 to 82331\n",
      "Data columns (total 45 columns):\n",
      "id                   82332 non-null int64\n",
      "dur                  82332 non-null float64\n",
      "proto                82332 non-null object\n",
      "service              82332 non-null object\n",
      "state                82332 non-null object\n",
      "spkts                82332 non-null int64\n",
      "dpkts                82332 non-null int64\n",
      "sbytes               82332 non-null int64\n",
      "dbytes               82332 non-null int64\n",
      "rate                 82332 non-null float64\n",
      "sttl                 82332 non-null int64\n",
      "dttl                 82332 non-null int64\n",
      "sload                82332 non-null float64\n",
      "dload                82332 non-null float64\n",
      "sloss                82332 non-null int64\n",
      "dloss                82332 non-null int64\n",
      "sinpkt               82332 non-null float64\n",
      "dinpkt               82332 non-null float64\n",
      "sjit                 82332 non-null float64\n",
      "djit                 82332 non-null float64\n",
      "swin                 82332 non-null int64\n",
      "stcpb                82332 non-null int64\n",
      "dtcpb                82332 non-null int64\n",
      "dwin                 82332 non-null int64\n",
      "tcprtt               82332 non-null float64\n",
      "synack               82332 non-null float64\n",
      "ackdat               82332 non-null float64\n",
      "smean                82332 non-null int64\n",
      "dmean                82332 non-null int64\n",
      "trans_depth          82332 non-null int64\n",
      "response_body_len    82332 non-null int64\n",
      "ct_srv_src           82332 non-null int64\n",
      "ct_state_ttl         82332 non-null int64\n",
      "ct_dst_ltm           82332 non-null int64\n",
      "ct_src_dport_ltm     82332 non-null int64\n",
      "ct_dst_sport_ltm     82332 non-null int64\n",
      "ct_dst_src_ltm       82332 non-null int64\n",
      "is_ftp_login         82332 non-null int64\n",
      "ct_ftp_cmd           82332 non-null int64\n",
      "ct_flw_http_mthd     82332 non-null int64\n",
      "ct_src_ltm           82332 non-null int64\n",
      "ct_srv_dst           82332 non-null int64\n",
      "is_sm_ips_ports      82332 non-null int64\n",
      "attack_cat           82332 non-null object\n",
      "label                82332 non-null int64\n",
      "dtypes: float64(11), int64(30), object(4)\n",
      "memory usage: 28.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate record deleted successfully: 82328 observations remaining\n"
     ]
    }
   ],
   "source": [
    "# Lets remove attributes that are not useful to us during this first analysis pass\n",
    "non_useful_features_list = ['id', 'attack_cat']\n",
    "#non_useful_features_list = ['id'] # Re,pved attacl_cat/ LR accuracy: 0.754888861897\n",
    "# id: n internal variable to just ref an obseration. deemed not usefl\n",
    "# attack_cat: first try and just predict the label. \n",
    "#             It will obviously 1:1 correlate with label\n",
    "#             We can circle back and swap it out with label \n",
    "#             to see if we get any better accuracy on an \n",
    "#             on an attack type level\n",
    "for feature in non_useful_features_list:\n",
    "    if feature in df:\n",
    "        df.drop(feature, axis=1, inplace=True)  # Lets drop id as it is an internal variable to just ref an obseratio\n",
    "\n",
    "# Overwrite the existing dataframe with the new dataframe that does not contain the \n",
    "# four unwanted records and confirm we have 4 less records (shold have 82328 observations)\n",
    "if \"is_ftp_login\" in df:\n",
    "    df = df[df.is_ftp_login != 2]\n",
    "    if len(df) == 82328:\n",
    "        print \"duplicate record deleted successfully: \" + str(len(df)) + \" observations remaining\" \n",
    "\n",
    "# Check to see if non useful features still exist in dataframe, if so, we did something wrong\n",
    "for feature in non_useful_features_list:\n",
    "    if feature in df:\n",
    "        print \"[\" + feature + \"]\" + \"still found, check removal code. (Should not see this)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Data columns (total 43 columns):\n",
      "dur                  82328 non-null float64\n",
      "proto                82328 non-null object\n",
      "service              82328 non-null object\n",
      "state                82328 non-null object\n",
      "spkts                82328 non-null int64\n",
      "dpkts                82328 non-null int64\n",
      "sbytes               82328 non-null int64\n",
      "dbytes               82328 non-null int64\n",
      "rate                 82328 non-null float64\n",
      "sttl                 82328 non-null int64\n",
      "dttl                 82328 non-null int64\n",
      "sload                82328 non-null float64\n",
      "dload                82328 non-null float64\n",
      "sloss                82328 non-null int64\n",
      "dloss                82328 non-null int64\n",
      "sinpkt               82328 non-null float64\n",
      "dinpkt               82328 non-null float64\n",
      "sjit                 82328 non-null float64\n",
      "djit                 82328 non-null float64\n",
      "swin                 82328 non-null int64\n",
      "stcpb                82328 non-null int64\n",
      "dtcpb                82328 non-null int64\n",
      "dwin                 82328 non-null int64\n",
      "tcprtt               82328 non-null float64\n",
      "synack               82328 non-null float64\n",
      "ackdat               82328 non-null float64\n",
      "smean                82328 non-null int64\n",
      "dmean                82328 non-null int64\n",
      "trans_depth          82328 non-null int64\n",
      "response_body_len    82328 non-null int64\n",
      "ct_srv_src           82328 non-null int64\n",
      "ct_state_ttl         82328 non-null int64\n",
      "ct_dst_ltm           82328 non-null int64\n",
      "ct_src_dport_ltm     82328 non-null int64\n",
      "ct_dst_sport_ltm     82328 non-null int64\n",
      "ct_dst_src_ltm       82328 non-null int64\n",
      "is_ftp_login         82328 non-null int64\n",
      "ct_ftp_cmd           82328 non-null int64\n",
      "ct_flw_http_mthd     82328 non-null int64\n",
      "ct_src_ltm           82328 non-null int64\n",
      "ct_srv_dst           82328 non-null int64\n",
      "is_sm_ips_ports      82328 non-null int64\n",
      "label                82328 non-null int64\n",
      "dtypes: float64(11), int64(29), object(3)\n",
      "memory usage: 27.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82328</td>\n",
       "      <td>82328</td>\n",
       "      <td>82328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>131</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>43091</td>\n",
       "      <td>47153</td>\n",
       "      <td>39335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        proto service  state\n",
       "count   82328   82328  82328\n",
       "unique    131      13      7\n",
       "top       tcp       -    FIN\n",
       "freq    43091   47153  39335"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=['O']) # Surround in try except incase there are no object type features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, lets one-hot encode the above categorical variables. Note expect number of\n",
    "features to increase based on sum of unique values. so $$43 + 131 + 13 + 7 - 3 = 191$$\n",
    "Must subtract $3$ for the 3 original colums that exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/19482970/get-list-from-pandas-dataframe-column-headers\n",
    "\n",
    "# Surrounding code in try/except on case where there are no object type features to one-hot encode\n",
    "try:\n",
    "    tmp_df = df.describe(include=['O'])  # creates a temporary df with just categorical features that are of object type\n",
    "    categorical_object_col_name_list = tmp_df.columns.values.tolist()\n",
    "    for col_name in categorical_object_col_name_list:\n",
    "        #print col_name\n",
    "        tmp_df = pd.get_dummies(df[col_name], prefix=col_name)\n",
    "        df = pd.concat((df,tmp_df), axis=1)\n",
    "        df.drop(col_name, axis=1, inplace=True)  # go ahead and drop original feature as it has now been one-hot encoded\n",
    "except ValueError as e:\n",
    "    print \"Value error({0}): \".format(e)  # Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Columns: 191 entries, dur to state_RST\n",
      "dtypes: float64(162), int64(29)\n",
      "memory usage: 120.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(82328, n_iter=3, test_size=0.2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "# we want to predict the X and y data as follows:\n",
    "if 'label' in df:\n",
    "    y = df['label'].values # get the labels we want\n",
    "    del df['label'] # get rid of the class label\n",
    "    X = df.values # use everything else to predict!\n",
    "\n",
    "    # X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    # have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "# of the object and set it up. This object will be able to split our data into \n",
    "# training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print cv_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.75877566  0.7575003   0.75634641]\n",
      "Average Accuracy across 3 shuffle split cross validation iterations = 0.757540791125\n"
     ]
    }
   ],
   "source": [
    "# first we create a reusable logisitic regression object\n",
    "# here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "print(accuracies)\n",
    "print \"Average Accuracy across \" + str(num_cv_iterations) + \" shuffle split cross validation iterations = \" + str(np.average(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76783871  0.76632052  0.77014635]\n",
      "Average Accuracy across 3 shuffle split cross validation iterations = 0.757540791125\n"
     ]
    }
   ],
   "source": [
    "# we want to predict the X and y data as follows for df_five:\n",
    "if 'label' in df_five:\n",
    "    y = df_five['label'].values # get the labels we want\n",
    "    del df_five['label'] # get rid of the class label\n",
    "    X = df_five.values # use everything else to predict!\n",
    "\n",
    "    # X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    # have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "# of the object and set it up. This object will be able to split our data into \n",
    "# training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances_five = len(y)\n",
    "cv_object_five = ShuffleSplit(n=num_instances_five,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "# first we create a reusable logisitic regression object\n",
    "# here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "\n",
    "accuracies_five = cross_val_score(lr_clf, X, y=y, cv=cv_object_five) # this also can help with parallelism\n",
    "print(accuracies_five)\n",
    "print \"Average Accuracy across \" + str(num_cv_iterations) + \" shuffle split cross validation iterations = \" + str(np.average(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
