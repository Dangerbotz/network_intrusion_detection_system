{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: SVM & LR Classification\n",
    "\n",
    "There are two natural dependent variables in our dataset: label and attack_cat.\n",
    "Initial approach is to prep the existing variables for classification and throw in the \"kitchen sink\" and use all features in the dataset to baseline our accuracy.  We then investigate what variables my be cross correlated, or less weight and remove them to see if we can increase our accuracy.\n",
    "\n",
    "In this mini-project, we decided upon trying two different LR classifications and two different SVM classifications. The first LR and SVM classification is on the full dataset, variable \"df\" in our code and referred to as the \"kitchen sink\" model. We then tried doing the LR and SVM classification on the five most correlated features to the label feature, our dependent variable. We'll refer to this one as the \"five-feature\" model, which is represented by the variable \"df_five\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "http://stats.stackexchange.com/questions/123632/how-does-support-vector-machine-compare-to-logistic-regression\n",
    "https://github.com/eclarson/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as mt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', None)    # set the max columns to show to unlimited\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82332 entries, 0 to 82331\n",
      "Data columns (total 6 columns):\n",
      "sttl                82332 non-null int64\n",
      "ct_dst_sport_ltm    82332 non-null int64\n",
      "ct_src_dport_ltm    82332 non-null int64\n",
      "swin                82332 non-null int64\n",
      "dwin                82332 non-null int64\n",
      "label               82332 non-null int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 3.8 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('UNSW_NB15_training_set.csv', encoding='utf-8-sig')  # specifing encoding to get rid of the UTF- Byte order Mark (BOM) in the id field\n",
    "df_five = pd.read_csv('miniLab.csv') # read in the five features most correlated to the label feature\n",
    "\n",
    "df_five.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82332 entries, 0 to 82331\n",
      "Data columns (total 45 columns):\n",
      "id                   82332 non-null int64\n",
      "dur                  82332 non-null float64\n",
      "proto                82332 non-null object\n",
      "service              82332 non-null object\n",
      "state                82332 non-null object\n",
      "spkts                82332 non-null int64\n",
      "dpkts                82332 non-null int64\n",
      "sbytes               82332 non-null int64\n",
      "dbytes               82332 non-null int64\n",
      "rate                 82332 non-null float64\n",
      "sttl                 82332 non-null int64\n",
      "dttl                 82332 non-null int64\n",
      "sload                82332 non-null float64\n",
      "dload                82332 non-null float64\n",
      "sloss                82332 non-null int64\n",
      "dloss                82332 non-null int64\n",
      "sinpkt               82332 non-null float64\n",
      "dinpkt               82332 non-null float64\n",
      "sjit                 82332 non-null float64\n",
      "djit                 82332 non-null float64\n",
      "swin                 82332 non-null int64\n",
      "stcpb                82332 non-null int64\n",
      "dtcpb                82332 non-null int64\n",
      "dwin                 82332 non-null int64\n",
      "tcprtt               82332 non-null float64\n",
      "synack               82332 non-null float64\n",
      "ackdat               82332 non-null float64\n",
      "smean                82332 non-null int64\n",
      "dmean                82332 non-null int64\n",
      "trans_depth          82332 non-null int64\n",
      "response_body_len    82332 non-null int64\n",
      "ct_srv_src           82332 non-null int64\n",
      "ct_state_ttl         82332 non-null int64\n",
      "ct_dst_ltm           82332 non-null int64\n",
      "ct_src_dport_ltm     82332 non-null int64\n",
      "ct_dst_sport_ltm     82332 non-null int64\n",
      "ct_dst_src_ltm       82332 non-null int64\n",
      "is_ftp_login         82332 non-null int64\n",
      "ct_ftp_cmd           82332 non-null int64\n",
      "ct_flw_http_mthd     82332 non-null int64\n",
      "ct_src_ltm           82332 non-null int64\n",
      "ct_srv_dst           82332 non-null int64\n",
      "is_sm_ips_ports      82332 non-null int64\n",
      "attack_cat           82332 non-null object\n",
      "label                82332 non-null int64\n",
      "dtypes: float64(11), int64(30), object(4)\n",
      "memory usage: 28.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate record deleted successfully: 82328 observations remaining\n"
     ]
    }
   ],
   "source": [
    "# Lets remove attributes that are not useful to us during this first analysis pass\n",
    "non_useful_features_list = ['id', 'attack_cat']\n",
    "#non_useful_features_list = ['id'] # Re,pved attacl_cat/ LR accuracy: 0.754888861897\n",
    "# id: n internal variable to just ref an obseration. deemed not usefl\n",
    "# attack_cat: first try and just predict the label. \n",
    "#             It will obviously 1:1 correlate with label\n",
    "#             We can circle back and swap it out with label \n",
    "#             to see if we get any better accuracy on an \n",
    "#             on an attack type level\n",
    "for feature in non_useful_features_list:\n",
    "    if feature in df:\n",
    "        df.drop(feature, axis=1, inplace=True)  # Lets drop id as it is an internal variable to just ref an obseratio\n",
    "\n",
    "# Overwrite the existing dataframe with the new dataframe that does not contain the \n",
    "# four unwanted records and confirm we have 4 less records (shold have 82328 observations)\n",
    "if \"is_ftp_login\" in df:\n",
    "    df = df[df.is_ftp_login != 2]\n",
    "    if len(df) == 82328:\n",
    "        print \"duplicate record deleted successfully: \" + str(len(df)) + \" observations remaining\" \n",
    "\n",
    "# Check to see if non useful features still exist in dataframe, if so, we did something wrong\n",
    "for feature in non_useful_features_list:\n",
    "    if feature in df:\n",
    "        print \"[\" + feature + \"]\" + \"still found, check removal code. (Should not see this)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Data columns (total 43 columns):\n",
      "dur                  82328 non-null float64\n",
      "proto                82328 non-null object\n",
      "service              82328 non-null object\n",
      "state                82328 non-null object\n",
      "spkts                82328 non-null int64\n",
      "dpkts                82328 non-null int64\n",
      "sbytes               82328 non-null int64\n",
      "dbytes               82328 non-null int64\n",
      "rate                 82328 non-null float64\n",
      "sttl                 82328 non-null int64\n",
      "dttl                 82328 non-null int64\n",
      "sload                82328 non-null float64\n",
      "dload                82328 non-null float64\n",
      "sloss                82328 non-null int64\n",
      "dloss                82328 non-null int64\n",
      "sinpkt               82328 non-null float64\n",
      "dinpkt               82328 non-null float64\n",
      "sjit                 82328 non-null float64\n",
      "djit                 82328 non-null float64\n",
      "swin                 82328 non-null int64\n",
      "stcpb                82328 non-null int64\n",
      "dtcpb                82328 non-null int64\n",
      "dwin                 82328 non-null int64\n",
      "tcprtt               82328 non-null float64\n",
      "synack               82328 non-null float64\n",
      "ackdat               82328 non-null float64\n",
      "smean                82328 non-null int64\n",
      "dmean                82328 non-null int64\n",
      "trans_depth          82328 non-null int64\n",
      "response_body_len    82328 non-null int64\n",
      "ct_srv_src           82328 non-null int64\n",
      "ct_state_ttl         82328 non-null int64\n",
      "ct_dst_ltm           82328 non-null int64\n",
      "ct_src_dport_ltm     82328 non-null int64\n",
      "ct_dst_sport_ltm     82328 non-null int64\n",
      "ct_dst_src_ltm       82328 non-null int64\n",
      "is_ftp_login         82328 non-null int64\n",
      "ct_ftp_cmd           82328 non-null int64\n",
      "ct_flw_http_mthd     82328 non-null int64\n",
      "ct_src_ltm           82328 non-null int64\n",
      "ct_srv_dst           82328 non-null int64\n",
      "is_sm_ips_ports      82328 non-null int64\n",
      "label                82328 non-null int64\n",
      "dtypes: float64(11), int64(29), object(3)\n",
      "memory usage: 27.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82328</td>\n",
       "      <td>82328</td>\n",
       "      <td>82328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>131</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>FIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>43091</td>\n",
       "      <td>47153</td>\n",
       "      <td>39335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        proto service  state\n",
       "count   82328   82328  82328\n",
       "unique    131      13      7\n",
       "top       tcp       -    FIN\n",
       "freq    43091   47153  39335"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=['O']) # Surround in try except incase there are no object type features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, lets one-hot encode the above categorical variables. Note expect number of\n",
    "features to increase based on sum of unique values. so $$43 + 131 + 13 + 7 - 3 = 191$$\n",
    "Must subtract $3$ for the 3 original colums that exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/19482970/get-list-from-pandas-dataframe-column-headers\n",
    "\n",
    "# Surrounding code in try/except on case where there are no object type features to one-hot encode\n",
    "try:\n",
    "    tmp_df = df.describe(include=['O'])  # creates a temporary df with just categorical features that are of object type\n",
    "    categorical_object_col_name_list = tmp_df.columns.values.tolist()\n",
    "    for col_name in categorical_object_col_name_list:\n",
    "        #print col_name\n",
    "        tmp_df = pd.get_dummies(df[col_name], prefix=col_name)\n",
    "        df = pd.concat((df,tmp_df), axis=1)\n",
    "        df.drop(col_name, axis=1, inplace=True)  # go ahead and drop original feature as it has now been one-hot encoded\n",
    "except ValueError as e:\n",
    "    print \"Value error({0}): \".format(e)  # Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Columns: 191 entries, dur to state_RST\n",
      "dtypes: float64(162), int64(29)\n",
      "memory usage: 120.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Now let's use Logistic Regression from `scikit-learn`. The documentation can be found here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(82328, n_iter=3, test_size=0.2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "# we want to predict the X and y data as follows:\n",
    "if 'label' in df:\n",
    "    y = df['label'].values # get the labels we want\n",
    "    del df['label'] # get rid of the class label\n",
    "    X = df.values # use everything else to predict!\n",
    "\n",
    "    # X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    # have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "# of the object and set it up. This object will be able to split our data into \n",
    "# training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print cv_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.75337058  0.75525325  0.75482813]\n",
      "Average Accuracy across 3 shuffle split cross validation iterations = 0.754483987206\n"
     ]
    }
   ],
   "source": [
    "# first we create a reusable logisitic regression object\n",
    "# here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "for train_indices, test_indices in cv_object:\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test)\n",
    "\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "print(accuracies)\n",
    "print \"Average Accuracy across \" + str(num_cv_iterations) + \" shuffle split cross validation iterations = \" + str(np.average(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76928216  0.76715657  0.76569902]\n",
      "Average Accuracy across 3 shuffle split cross validation iterations = 0.767379246123\n"
     ]
    }
   ],
   "source": [
    "# we want to predict the X and y data as follows for df_five:\n",
    "if 'label' in df_five:\n",
    "    y_five = df_five['label'].values # get the labels we want\n",
    "    del df_five['label'] # get rid of the class label\n",
    "    X_five = df_five.values # use everything else to predict!\n",
    "\n",
    "    # X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    # have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "# of the object and set it up. This object will be able to split our data into \n",
    "# training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances_five = len(y)\n",
    "cv_object_five = ShuffleSplit(n=num_instances_five,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "# here we can setup the object with different learning parameters and constants\n",
    "lr_clf_five = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "for train_indices, test_indices in cv_object_five:\n",
    "    X_train_five = X[train_indices]\n",
    "    y_train_five = y[train_indices]\n",
    "    \n",
    "    X_test_five = X[test_indices]\n",
    "    y_test_five = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf_five.fit(X_train_five, y_train_five)  # train object\n",
    "    y_hat_five = lr_clf_five.predict(X_test_five)\n",
    "    \n",
    "accuracies_five = cross_val_score(lr_clf_five, X_five, y=y_five, cv=cv_object_five) # this also can help with parallelism\n",
    "print(accuracies_five)\n",
    "print \"Average Accuracy across \" + str(num_cv_iterations) + \" shuffle split cross validation iterations = \" + str(np.average(accuracies_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried \"`l1`\" and \"`l2`\", a '`C`' as high as 2.0 and as low as 0.05, and `class_weight` of \"None\" and \"Balanced\" and still got the best accuracy with the this `lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None)` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "    We see that the five-feature model performs slightly better (one percent on average) than the kitchen sink model, 76.8% to 75.3%, respectively. Interpretting the weights of each model may give us insight as to why there is a difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretting weights\n",
    "Okay, so now lets take the models for logistic regression and try to interpret the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'dur', 'has weight of', 8.68028522203702e-11)\n",
      "(u'spkts', 'has weight of', 9.9251135755035939e-10)\n",
      "(u'dpkts', 'has weight of', 1.5465338391157435e-09)\n",
      "(u'sbytes', 'has weight of', 1.2033775050618821e-06)\n",
      "(u'dbytes', 'has weight of', 2.335357996058891e-06)\n",
      "(u'rate', 'has weight of', 9.1317862452710543e-06)\n",
      "(u'sttl', 'has weight of', 2.5011956271733406e-08)\n",
      "(u'dttl', 'has weight of', 6.3909157966725779e-09)\n",
      "(u'sload', 'has weight of', -1.8773042163582781e-09)\n",
      "(u'dload', 'has weight of', -2.7441702712699513e-06)\n",
      "(u'sloss', 'has weight of', 3.8852143902106041e-10)\n",
      "(u'dloss', 'has weight of', 8.7355995193033847e-10)\n",
      "(u'sinpkt', 'has weight of', -4.3227869279958647e-07)\n",
      "(u'dinpkt', 'has weight of', -1.6362855068104674e-08)\n",
      "(u'sjit', 'has weight of', -1.581296867364659e-07)\n",
      "(u'djit', 'has weight of', 2.0084064039401869e-08)\n",
      "(u'swin', 'has weight of', -5.3377528046987095e-09)\n",
      "(u'stcpb', 'has weight of', -2.1194158561685808e-11)\n",
      "(u'dtcpb', 'has weight of', -3.2455592396256935e-11)\n",
      "(u'dwin', 'has weight of', -1.019976862874151e-09)\n",
      "(u'tcprtt', 'has weight of', -4.605256590144888e-12)\n",
      "(u'synack', 'has weight of', -3.5468553271223033e-12)\n",
      "(u'ackdat', 'has weight of', -1.0584012630226002e-12)\n",
      "(u'smean', 'has weight of', 2.5591511130281564e-09)\n",
      "(u'dmean', 'has weight of', 1.453135491926664e-08)\n",
      "(u'trans_depth', 'has weight of', 7.8230657238292939e-12)\n",
      "(u'response_body_len', 'has weight of', 2.610083323020877e-07)\n",
      "(u'ct_srv_src', 'has weight of', 1.5693912184103799e-09)\n",
      "(u'ct_state_ttl', 'has weight of', 1.1408529398367517e-10)\n",
      "(u'ct_dst_ltm', 'has weight of', 1.0584676508306068e-09)\n",
      "(u'ct_src_dport_ltm', 'has weight of', 1.2895907992662204e-09)\n",
      "(u'ct_dst_sport_ltm', 'has weight of', 1.067708301282111e-09)\n",
      "(u'ct_dst_src_ltm', 'has weight of', 1.2399822761993554e-09)\n",
      "(u'is_ftp_login', 'has weight of', 3.8826564597001926e-13)\n",
      "(u'ct_ftp_cmd', 'has weight of', 3.4154267933549821e-13)\n",
      "(u'ct_flw_http_mthd', 'has weight of', -9.2249644567687895e-12)\n",
      "(u'ct_src_ltm', 'has weight of', 1.1971578216489056e-09)\n",
      "(u'ct_srv_dst', 'has weight of', 1.5605186318317348e-09)\n",
      "(u'is_sm_ips_ports', 'has weight of', -7.2870125094805439e-12)\n",
      "(u'proto_3pc', 'has weight of', 2.1114206390589958e-13)\n",
      "(u'proto_a/n', 'has weight of', 7.6418931317439674e-14)\n",
      "(u'proto_aes-sp3-d', 'has weight of', 1.8029535725836371e-13)\n",
      "(u'proto_any', 'has weight of', 4.5238296583053326e-13)\n",
      "(u'proto_argus', 'has weight of', 1.8342143201105898e-13)\n",
      "(u'proto_aris', 'has weight of', 1.8515310476991037e-13)\n",
      "(u'proto_arp', 'has weight of', -7.8422623613251974e-12)\n",
      "(u'proto_ax.25', 'has weight of', 1.7478596563820434e-13)\n",
      "(u'proto_bbn-rcc', 'has weight of', 1.3374074766604242e-13)\n",
      "(u'proto_bna', 'has weight of', 1.1845197938611333e-13)\n",
      "(u'proto_br-sat-mon', 'has weight of', 1.9608072396888311e-13)\n",
      "(u'proto_cbt', 'has weight of', 2.5454263239642654e-13)\n",
      "(u'proto_cftp', 'has weight of', 2.012531598170117e-13)\n",
      "(u'proto_chaos', 'has weight of', 1.6311051413273426e-13)\n",
      "(u'proto_compaq-peer', 'has weight of', 1.8215662910835367e-13)\n",
      "(u'proto_cphb', 'has weight of', 1.242714576123727e-13)\n",
      "(u'proto_cpnx', 'has weight of', 2.0377962913100425e-13)\n",
      "(u'proto_crtp', 'has weight of', 1.0720183584501531e-13)\n",
      "(u'proto_crudp', 'has weight of', 8.0531610850820552e-14)\n",
      "(u'proto_dcn', 'has weight of', 1.6007828399369428e-13)\n",
      "(u'proto_ddp', 'has weight of', 1.6380909120146627e-13)\n",
      "(u'proto_ddx', 'has weight of', 1.2243779110575556e-13)\n",
      "(u'proto_dgp', 'has weight of', 1.1844029319870989e-13)\n",
      "(u'proto_egp', 'has weight of', 1.2817566369562216e-13)\n",
      "(u'proto_eigrp', 'has weight of', 1.1650901819412384e-13)\n",
      "(u'proto_emcon', 'has weight of', 1.1255946537627494e-13)\n",
      "(u'proto_encap', 'has weight of', 1.409853725212584e-13)\n",
      "(u'proto_etherip', 'has weight of', 1.0600313902634866e-13)\n",
      "(u'proto_fc', 'has weight of', 1.9780497407297347e-13)\n",
      "(u'proto_fire', 'has weight of', 7.2269240539418053e-14)\n",
      "(u'proto_ggp', 'has weight of', 1.4716293159945089e-13)\n",
      "(u'proto_gmtp', 'has weight of', 1.6380667373063661e-13)\n",
      "(u'proto_gre', 'has weight of', 4.7212345940763687e-13)\n",
      "(u'proto_hmp', 'has weight of', 1.7259645135733014e-13)\n",
      "(u'proto_i-nlsp', 'has weight of', 1.3362016379392546e-13)\n",
      "(u'proto_iatp', 'has weight of', 1.6338833985453821e-13)\n",
      "(u'proto_ib', 'has weight of', 2.0586929894135807e-13)\n",
      "(u'proto_idpr', 'has weight of', 1.0605049696355192e-13)\n",
      "(u'proto_idpr-cmtp', 'has weight of', 1.5129501618776289e-13)\n",
      "(u'proto_idrp', 'has weight of', 1.4687139759533692e-13)\n",
      "(u'proto_ifmp', 'has weight of', 1.0794785602499604e-13)\n",
      "(u'proto_igmp', 'has weight of', -2.415710662952698e-13)\n",
      "(u'proto_igp', 'has weight of', 1.484516078846311e-13)\n",
      "(u'proto_il', 'has weight of', 2.1576866534768112e-13)\n",
      "(u'proto_ip', 'has weight of', 1.184900928004547e-13)\n",
      "(u'proto_ipcomp', 'has weight of', 1.8427448202220306e-13)\n",
      "(u'proto_ipcv', 'has weight of', 9.0750823723090796e-14)\n",
      "(u'proto_ipip', 'has weight of', 1.9355425442728185e-13)\n",
      "(u'proto_iplt', 'has weight of', 1.5382816177315092e-13)\n",
      "(u'proto_ipnip', 'has weight of', 1.1897398078245756e-13)\n",
      "(u'proto_ippc', 'has weight of', 1.0547051219136488e-13)\n",
      "(u'proto_ipv6', 'has weight of', 2.3301692536601898e-13)\n",
      "(u'proto_ipv6-frag', 'has weight of', 1.8144381299996983e-13)\n",
      "(u'proto_ipv6-no', 'has weight of', 1.3934308237508304e-13)\n",
      "(u'proto_ipv6-opts', 'has weight of', 1.3186272727677706e-13)\n",
      "(u'proto_ipv6-route', 'has weight of', 1.7093330306102333e-13)\n",
      "(u'proto_ipx-n-ip', 'has weight of', 1.6189691140083995e-13)\n",
      "(u'proto_irtp', 'has weight of', 1.3848737105691976e-13)\n",
      "(u'proto_isis', 'has weight of', 1.2556106063949645e-13)\n",
      "(u'proto_iso-ip', 'has weight of', 8.3436803669874526e-14)\n",
      "(u'proto_iso-tp4', 'has weight of', 1.5129501652869136e-13)\n",
      "(u'proto_kryptolan', 'has weight of', 1.5655133811420552e-13)\n",
      "(u'proto_l2tp', 'has weight of', 2.0137912736353925e-13)\n",
      "(u'proto_larp', 'has weight of', 1.7694733284730178e-13)\n",
      "(u'proto_leaf-1', 'has weight of', 9.6530013949941516e-14)\n",
      "(u'proto_leaf-2', 'has weight of', 1.5752485708985969e-13)\n",
      "(u'proto_merit-inp', 'has weight of', 1.3622520275033807e-13)\n",
      "(u'proto_mfe-nsp', 'has weight of', 1.330110496878482e-13)\n",
      "(u'proto_mhrp', 'has weight of', 1.7903874891205027e-13)\n",
      "(u'proto_micp', 'has weight of', 1.2670445872583445e-13)\n",
      "(u'proto_mobile', 'has weight of', 3.0529845607265027e-13)\n",
      "(u'proto_mtp', 'has weight of', 9.6402639200998524e-14)\n",
      "(u'proto_mux', 'has weight of', 2.5454102881172993e-13)\n",
      "(u'proto_narp', 'has weight of', 1.7586857569921947e-13)\n",
      "(u'proto_netblt', 'has weight of', 2.0806975933052291e-13)\n",
      "(u'proto_nsfnet-igp', 'has weight of', 1.4250698582599479e-13)\n",
      "(u'proto_nvp', 'has weight of', 1.7307209448060443e-13)\n",
      "(u'proto_ospf', 'has weight of', 4.5810659914952934e-12)\n",
      "(u'proto_pgm', 'has weight of', 1.7405118326520642e-13)\n",
      "(u'proto_pim', 'has weight of', 2.8756905120528734e-13)\n",
      "(u'proto_pipe', 'has weight of', 1.6026326957051751e-13)\n",
      "(u'proto_pnni', 'has weight of', 1.6486779339873811e-13)\n",
      "(u'proto_pri-enc', 'has weight of', 1.1893641666294165e-13)\n",
      "(u'proto_prm', 'has weight of', 1.7217373499473349e-13)\n",
      "(u'proto_ptp', 'has weight of', 1.4250698535443843e-13)\n",
      "(u'proto_pup', 'has weight of', 1.0762777411844548e-13)\n",
      "(u'proto_pvp', 'has weight of', 2.1530322244962019e-13)\n",
      "(u'proto_qnx', 'has weight of', 1.2855105264760576e-13)\n",
      "(u'proto_rdp', 'has weight of', 5.536880541031427e-14)\n",
      "(u'proto_rsvp', 'has weight of', 1.9273392661062657e-13)\n",
      "(u'proto_rvd', 'has weight of', 2.5459720903691941e-13)\n",
      "(u'proto_sat-expak', 'has weight of', 1.5094369855024451e-13)\n",
      "(u'proto_sat-mon', 'has weight of', 1.8745503212919169e-13)\n",
      "(u'proto_sccopmce', 'has weight of', 1.3598378158360448e-13)\n",
      "(u'proto_scps', 'has weight of', 1.4490748720809589e-13)\n",
      "(u'proto_sctp', 'has weight of', 2.008921902788649e-12)\n",
      "(u'proto_sdrp', 'has weight of', 1.7195659946961717e-13)\n",
      "(u'proto_secure-vmtp', 'has weight of', 1.6476481209364563e-13)\n",
      "(u'proto_sep', 'has weight of', 2.7575650348066659e-13)\n",
      "(u'proto_skip', 'has weight of', 1.9399540743819005e-13)\n",
      "(u'proto_sm', 'has weight of', 1.2776236713999427e-13)\n",
      "(u'proto_smp', 'has weight of', 1.6690629360827105e-13)\n",
      "(u'proto_snp', 'has weight of', 1.0743082410884021e-13)\n",
      "(u'proto_sprite-rpc', 'has weight of', 1.5740284597447034e-13)\n",
      "(u'proto_sps', 'has weight of', 1.4296896992229351e-13)\n",
      "(u'proto_srp', 'has weight of', 1.1711120917191104e-13)\n",
      "(u'proto_st2', 'has weight of', 2.9370164137931318e-13)\n",
      "(u'proto_stp', 'has weight of', 7.4426144692138308e-14)\n",
      "(u'proto_sun-nd', 'has weight of', 3.8457236563379768e-13)\n",
      "(u'proto_swipe', 'has weight of', 3.5561524059342182e-13)\n",
      "(u'proto_tcf', 'has weight of', 1.115962641274902e-13)\n",
      "(u'proto_tcp', 'has weight of', -2.097548696414375e-11)\n",
      "(u'proto_tlsp', 'has weight of', 1.3937712717817821e-13)\n",
      "(u'proto_tp++', 'has weight of', 1.9534386473014066e-13)\n",
      "(u'proto_trunk-1', 'has weight of', 1.6489754401716362e-13)\n",
      "(u'proto_trunk-2', 'has weight of', 1.7682261254283617e-13)\n",
      "(u'proto_ttp', 'has weight of', 1.4978365404085376e-13)\n",
      "(u'proto_udp', 'has weight of', 5.2440063351803035e-11)\n",
      "(u'proto_unas', 'has weight of', 1.6970885219795338e-11)\n",
      "(u'proto_uti', 'has weight of', 1.5084071637133201e-13)\n",
      "(u'proto_vines', 'has weight of', 1.6421977790424418e-13)\n",
      "(u'proto_visa', 'has weight of', 7.9763543510239189e-14)\n",
      "(u'proto_vmtp', 'has weight of', 1.7995616918731034e-13)\n",
      "(u'proto_vrrp', 'has weight of', 1.9493732537572662e-13)\n",
      "(u'proto_wb-expak', 'has weight of', 1.6490170978464874e-13)\n",
      "(u'proto_wb-mon', 'has weight of', 1.7245730472299797e-13)\n",
      "(u'proto_wsn', 'has weight of', 1.2593459587929298e-13)\n",
      "(u'proto_xnet', 'has weight of', 1.4143192918688729e-13)\n",
      "(u'proto_xns-idp', 'has weight of', 1.1096479242335562e-13)\n",
      "(u'proto_xtp', 'has weight of', 1.7656112426956837e-13)\n",
      "(u'proto_zero', 'has weight of', 1.9848578850956085e-13)\n",
      "(u'service_-', 'has weight of', -3.5959345117550929e-11)\n",
      "(u'service_dhcp', 'has weight of', 1.9580546700175507e-13)\n",
      "(u'service_dns', 'has weight of', 7.8816231044234822e-11)\n",
      "(u'service_ftp', 'has weight of', 1.5287059666207629e-12)\n",
      "(u'service_ftp-data', 'has weight of', -4.2385496428519879e-13)\n",
      "(u'service_http', 'has weight of', 1.2664126279334541e-11)\n",
      "(u'service_irc', 'has weight of', 4.2373428056817292e-14)\n",
      "(u'service_pop3', 'has weight of', 4.3365757545839842e-12)\n",
      "(u'service_radius', 'has weight of', 2.9298838036185914e-14)\n",
      "(u'service_smtp', 'has weight of', 6.7462922350786339e-12)\n",
      "(u'service_snmp', 'has weight of', 1.4609777009284813e-13)\n",
      "(u'service_ssh', 'has weight of', -8.5965217094458511e-13)\n",
      "(u'service_ssl', 'has weight of', 2.1048545974009033e-13)\n",
      "(u'state_ACC', 'has weight of', 8.795760395956672e-15)\n",
      "(u'state_CLO', 'has weight of', 1.0423828708787159e-14)\n",
      "(u'state_CON', 'has weight of', -2.7269048574275866e-11)\n",
      "(u'state_FIN', 'has weight of', 9.8837055825703198e-12)\n",
      "(u'state_INT', 'has weight of', 1.0111679439918703e-10)\n",
      "(u'state_REQ', 'has weight of', -1.6267767554032943e-11)\n",
      "(u'state_RST', 'has weight of', -9.7634525516081585e-15)\n"
     ]
    }
   ],
   "source": [
    "# Kitchen Sink Model\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df.columns\n",
    "weight_list = []\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sttl', 'has weight of', 5.751360385208197e-11)\n",
      "('ct_dst_sport_ltm', 'has weight of', 7.7476185500195847e-10)\n",
      "('ct_src_dport_ltm', 'has weight of', 1.107544085374592e-09)\n",
      "('swin', 'has weight of', 8.2713421852892032e-07)\n",
      "('dwin', 'has weight of', 1.6031127070757823e-06)\n"
     ]
    }
   ],
   "source": [
    "# Five-feature Model\n",
    "weights = lr_clf_five.coef_.T # take transpose to make a column vector\n",
    "variable_names = df_five.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weight interpretations are not neccessarily interpretable because of the values we had. Very large/small attribute values could just as easily be assigned a higher weight. Instead, let's normalize the feature values so that all the attributes are on the same dynamic range. Once we normalize the attributes, the weights should have magnitudes that reflect their predictive power in the logistic regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy:', 0.91874164945949233)\n",
      "[[6790  639]\n",
      " [ 699 8338]]\n",
      "top features based on largest postive weights (normalized) (Largest to smallest)\n",
      "(u'dttl', 4.1113494938565545)\n",
      "top features based on largest negative weights (normalized) (Smallest to Largest)\n",
      "(u'ct_dst_src_ltm', -4.2485540523456828)\n"
     ]
    }
   ],
   "source": [
    "# Kitchen Sink Model\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df.columns) # combine attributes\n",
    "zip_vars.sort(key = lambda t: np.abs(t[0])) # sort them by the magnitude of the weight\n",
    "weight_list_normalized = []\n",
    "for coef, name in zip_vars:\n",
    "    #print(name, 'has weight of', coef[0]) # now print them out\n",
    "    weight_list_normalized.append((name, coef[0]))  \n",
    "    \n",
    "    \n",
    "weight_list_normalized.sort(key=lambda x:x[1])\n",
    "print \"top features based on largest postive weights (normalized) (Largest to smallest)\"\n",
    "print weight_list_normalized[-1]\n",
    "\n",
    "print \"top features based on largest negative weights (normalized) (Smallest to Largest)\"\n",
    "print weight_list_normalized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ae2ecd9e7ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrained_model_from_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtemp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtemp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl), ('logit_model', lr_clf)])\n",
    "\n",
    "# run the pipline corssvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    \n",
    "# it is a little odd getting trained objects from a  pipeline:\n",
    "trained_model_from_pipeline = piped_object.named_steps['logit_model']\n",
    "\n",
    "# now look at the weights\n",
    "weights = pd.Series(trained_model_from_pipeline.coef_[0],index=df.columns)\n",
    "temp_df = pd.DataFrame(weights)\n",
    "temp_df = temp_df[(temp_df[0] > threshold) | (temp_df[0] < (-1 * threshold))]\n",
    "weights = temp_df[0]\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy:', 0.91983481112595655)\n",
      "[[6787  644]\n",
      " [ 676 8359]]\n",
      "top features based on largest postive weights (normalized) (Largest to smallest)\n",
      "('ct_src_dport_ltm', 0.84148426654843544)\n",
      "top features based on largest negative weights (normalized) (Smallest to Largest)\n",
      "('dwin', -1.2041302578363196)\n"
     ]
    }
   ],
   "source": [
    "# Five-Feature Model\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train_five) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train_five) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test_five) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train_five)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test_five,y_hat)\n",
    "conf = mt.confusion_matrix(y_test_five,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df_five.columns) # combine attributes\n",
    "zip_vars.sort(key = lambda t: np.abs(t[0])) # sort them by the magnitude of the weight\n",
    "weight_list_normalized = []\n",
    "for coef, name in zip_vars:\n",
    "    weight_list_normalized.append((name, coef[0]))  \n",
    "    \n",
    "    \n",
    "weight_list_normalized.sort(key=lambda x:x[1])\n",
    "print \"top features based on largest postive weights (normalized) (Largest to smallest)\"\n",
    "print weight_list_normalized[-1]\n",
    "\n",
    "print \"top features based on largest negative weights (normalized) (Smallest to Largest)\"\n",
    "print weight_list_normalized[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Logistic Regression are: 1) provides various ways to regularize a statistical model to prevent overfitting of the training model. 2) feature correlation is not as important. 3) provides a probabilistic interpretation. 4) allows for a model to be easily updated with new data. 5) can be used when the number of features is large and the training examples are small.\n",
    "\n",
    "Advantages of Support Vector Machines are: 1) offers theoretical guarantees against overfitting the model. 2) can be used when the data is not linearly separable, using the appropriate kernel. 3) can be used when the number of features are smaller than the number of training examples, using the Gaussian or RBF kernel. 4) outperforms at identifying complex data boundaries.  5) generalizes well once the hyperplane is determined."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
