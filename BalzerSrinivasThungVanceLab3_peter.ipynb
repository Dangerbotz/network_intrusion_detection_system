{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Network Intrusion Detection\n",
    "- Daniel Vance\n",
    "- Peter Thung\n",
    "- Ravi Srinivas\n",
    "- Randy Balzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Describe the purpose of the data set you selected. How will you measure the effectiveness of a good algorithm? Why does your chosen validation method make sense for this specific data set and the stakeholders' needs?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?*\n",
    "- *Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "# Imports\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.decomposition import RandomizedPCA \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of unique attack categories', 10)\n",
      "duplicate record deleted successfully: 82328 observations remaining\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Data columns (total 6 columns):\n",
      "sttl                82328 non-null int64\n",
      "ct_dst_sport_ltm    82328 non-null int64\n",
      "ct_src_dport_ltm    82328 non-null int64\n",
      "swin                82328 non-null int64\n",
      "dwin                82328 non-null int64\n",
      "label               82328 non-null int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 4.4 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sttl</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>swin</th>\n",
       "      <th>dwin</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82328.000000</td>\n",
       "      <td>82328.000000</td>\n",
       "      <td>82328.000000</td>\n",
       "      <td>82328.000000</td>\n",
       "      <td>82328.000000</td>\n",
       "      <td>82328.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>180.973448</td>\n",
       "      <td>3.663092</td>\n",
       "      <td>4.929040</td>\n",
       "      <td>133.453175</td>\n",
       "      <td>128.280464</td>\n",
       "      <td>0.550578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>101.512436</td>\n",
       "      <td>5.915518</td>\n",
       "      <td>8.389724</td>\n",
       "      <td>127.357276</td>\n",
       "      <td>127.491408</td>\n",
       "      <td>0.497438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>254.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>254.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>255.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sttl  ct_dst_sport_ltm  ct_src_dport_ltm          swin  \\\n",
       "count  82328.000000      82328.000000      82328.000000  82328.000000   \n",
       "mean     180.973448          3.663092          4.929040    133.453175   \n",
       "std      101.512436          5.915518          8.389724    127.357276   \n",
       "min        0.000000          1.000000          1.000000      0.000000   \n",
       "25%       62.000000          1.000000          1.000000      0.000000   \n",
       "50%      254.000000          1.000000          1.000000    255.000000   \n",
       "75%      254.000000          3.000000          4.000000    255.000000   \n",
       "max      255.000000         38.000000         59.000000    255.000000   \n",
       "\n",
       "               dwin         label  \n",
       "count  82328.000000  82328.000000  \n",
       "mean     128.280464      0.550578  \n",
       "std      127.491408      0.497438  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      0.000000  \n",
       "50%      255.000000      1.000000  \n",
       "75%      255.000000      1.000000  \n",
       "max      255.000000      1.000000  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load UNSW_NB15 into a Pandas dataframe\n",
    "df = pd.read_csv('UNSW_NB15_training_set.csv', encoding='utf-8-sig')\n",
    "# Grab out the count of number of unique values in the attack_cat variable prior to removing it\n",
    "# to use as a potential K value for Kmeans clustering algorithm\n",
    "\n",
    "attack_cat = df.attack_cat.unique()\n",
    "attack_cat_count = len(attack_cat)\n",
    "print(\"Number of unique attack categories\", attack_cat_count)\n",
    "# Lets remove attributes that are not useful to us during this first analysis pass\n",
    "non_useful_features_list = ['id', 'attack_cat']\n",
    "# id: n internal variable to just ref an obseration. deemed not usefl\n",
    "# attack_cat: first try and just predict the label. \n",
    "#             It will obviously 1:1 correlate with label\n",
    "#             We can circle back and swap it out with label \n",
    "#             to see if we get any better accuracy on an \n",
    "#             on an attack type level\n",
    "for feature in non_useful_features_list:\n",
    "    if feature in df:\n",
    "        df.drop(feature, axis=1, inplace=True)  # Lets drop id as it is an internal variable to just ref an obseratio\n",
    "        \n",
    "# Overwrite the existing dataframe with the new dataframe that does not contain the \n",
    "# four unwanted records and confirm we have 4 less records (shold have 82328 observations)\n",
    "if \"is_ftp_login\" in df:\n",
    "    df = df[df.is_ftp_login != 2]\n",
    "    if len(df) == 82328:\n",
    "        print \"duplicate record deleted successfully: \" + str(len(df)) + \" observations remaining\" \n",
    "# Check to see if non useful features still exist in dataframe, if so, we did something wrong\n",
    "for feature in non_useful_features_list:\n",
    "    if feature in df:\n",
    "        print \"[\" + feature + \"]\" + \"still found, check removal code. (Should not see this)\"      \n",
    "\n",
    "df_five = df[['sttl','ct_dst_sport_ltm', 'ct_src_dport_ltm', 'swin', 'dwin', 'label' ]] \n",
    "\n",
    "df_five.info()\n",
    "df_five.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation:¶\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value error(No objects to concatenate): \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Columns: 191 entries, dur to state_RST\n",
      "dtypes: float64(162), int64(29)\n",
      "memory usage: 120.6 MB\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode our object features:\n",
    "\n",
    "# http://stackoverflow.com/questions/19482970/get-list-from-pandas-dataframe-column-headers\n",
    "# Surrounding code in try/except on case where there are no object type features to one-hot encode\n",
    "try:\n",
    "    tmp_df = df.describe(include=['O'])  # creates a temporary df with just categorical features that are of object type\n",
    "    categorical_object_col_name_list = tmp_df.columns.values.tolist()\n",
    "    for col_name in categorical_object_col_name_list:\n",
    "        #print col_name\n",
    "        tmp_df = pd.get_dummies(df[col_name], prefix=col_name)\n",
    "        df = pd.concat((df,tmp_df), axis=1)\n",
    "        df.drop(col_name, axis=1, inplace=True)  # go ahead and drop original feature as it has now been one-hot encoded\n",
    "except ValueError as e:\n",
    "    print \"Value error({0}): \".format(e)  # Note\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82328 entries, 0 to 82331\n",
      "Columns: 191 entries, dur to state_RST\n",
      "dtypes: float64(162), int64(29)\n",
      "memory usage: 120.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 412 entries, 71893 to 28318\n",
      "Columns: 190 entries, dur to state_RST\n",
      "dtypes: float64(162), int64(28)\n",
      "memory usage: 614.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Go ahead and drop both 'label' \n",
    "dfcopy = df.copy(deep=True) # preserve original dataframe that has our dependent variable\n",
    "dfcopy.info()\n",
    "#http://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.DataFrame.sample.html\n",
    "dfCopyHalf = dfcopy.sample(frac=.005)\n",
    "dfcopy = dfCopyHalf\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'label' in dfcopy:\n",
    "    y = dfcopy['label'].values # get the labels we want\n",
    "    del dfcopy['label'] # get rid of the class label\n",
    "    X = dfcopy.values # use everything else to cluster!\n",
    "dfcopy.info() # should have 190 featurers, ~40k entries\n",
    "#http://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.DataFrame.sample.html\n",
    "\n",
    "    # X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    # have converted them into simple matrices to use with scikit learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>sinpkt</th>\n",
       "      <th>dinpkt</th>\n",
       "      <th>sjit</th>\n",
       "      <th>djit</th>\n",
       "      <th>swin</th>\n",
       "      <th>stcpb</th>\n",
       "      <th>dtcpb</th>\n",
       "      <th>dwin</th>\n",
       "      <th>tcprtt</th>\n",
       "      <th>synack</th>\n",
       "      <th>ackdat</th>\n",
       "      <th>smean</th>\n",
       "      <th>dmean</th>\n",
       "      <th>trans_depth</th>\n",
       "      <th>response_body_len</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_state_ttl</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>4.120000e+02</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>4.120000e+02</td>\n",
       "      <td>4.120000e+02</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>412.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.802402</td>\n",
       "      <td>17.798544</td>\n",
       "      <td>40.186893</td>\n",
       "      <td>4061.817961</td>\n",
       "      <td>43975.111650</td>\n",
       "      <td>95889.921817</td>\n",
       "      <td>180.293689</td>\n",
       "      <td>91.041262</td>\n",
       "      <td>8.269606e+07</td>\n",
       "      <td>689131.999606</td>\n",
       "      <td>3.201456</td>\n",
       "      <td>17.939320</td>\n",
       "      <td>388.406615</td>\n",
       "      <td>96.163556</td>\n",
       "      <td>6188.389576</td>\n",
       "      <td>462.261406</td>\n",
       "      <td>133.689320</td>\n",
       "      <td>1.097451e+09</td>\n",
       "      <td>9.519038e+08</td>\n",
       "      <td>125.643204</td>\n",
       "      <td>0.056769</td>\n",
       "      <td>0.027610</td>\n",
       "      <td>0.029159</td>\n",
       "      <td>159.179612</td>\n",
       "      <td>109.565534</td>\n",
       "      <td>0.084951</td>\n",
       "      <td>1343.293689</td>\n",
       "      <td>9.881068</td>\n",
       "      <td>1.381068</td>\n",
       "      <td>5.854369</td>\n",
       "      <td>4.813107</td>\n",
       "      <td>3.490291</td>\n",
       "      <td>7.611650</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>0.126214</td>\n",
       "      <td>6.359223</td>\n",
       "      <td>9.550971</td>\n",
       "      <td>0.007282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.955333</td>\n",
       "      <td>83.727403</td>\n",
       "      <td>537.250046</td>\n",
       "      <td>17026.318119</td>\n",
       "      <td>721052.362075</td>\n",
       "      <td>178648.222057</td>\n",
       "      <td>102.195241</td>\n",
       "      <td>114.941647</td>\n",
       "      <td>2.251225e+08</td>\n",
       "      <td>2412412.100645</td>\n",
       "      <td>7.535525</td>\n",
       "      <td>268.617491</td>\n",
       "      <td>4218.624716</td>\n",
       "      <td>875.950877</td>\n",
       "      <td>60571.784329</td>\n",
       "      <td>2437.142307</td>\n",
       "      <td>127.504518</td>\n",
       "      <td>1.402489e+09</td>\n",
       "      <td>1.278484e+09</td>\n",
       "      <td>127.641478</td>\n",
       "      <td>0.145878</td>\n",
       "      <td>0.071078</td>\n",
       "      <td>0.084931</td>\n",
       "      <td>233.105287</td>\n",
       "      <td>232.349898</td>\n",
       "      <td>0.279148</td>\n",
       "      <td>19574.748404</td>\n",
       "      <td>11.039541</td>\n",
       "      <td>1.151718</td>\n",
       "      <td>7.960557</td>\n",
       "      <td>7.908019</td>\n",
       "      <td>5.594784</td>\n",
       "      <td>11.341617</td>\n",
       "      <td>0.109626</td>\n",
       "      <td>0.109626</td>\n",
       "      <td>0.611004</td>\n",
       "      <td>7.919122</td>\n",
       "      <td>11.108613</td>\n",
       "      <td>0.085124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.864416</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.258315e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.005863</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>2798.509862</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>6.278774e+05</td>\n",
       "      <td>1838.408874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479206</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>16.318723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.663749</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1540.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>125000.000300</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>8.888889e+07</td>\n",
       "      <td>23318.995120</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.984806</td>\n",
       "      <td>55.786144</td>\n",
       "      <td>3026.191321</td>\n",
       "      <td>110.168092</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>2.191444e+09</td>\n",
       "      <td>1.822121e+09</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.103908</td>\n",
       "      <td>0.046738</td>\n",
       "      <td>0.046070</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.983105</td>\n",
       "      <td>1642.000000</td>\n",
       "      <td>10872.000000</td>\n",
       "      <td>185550.000000</td>\n",
       "      <td>14590859.000000</td>\n",
       "      <td>1000000.003000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>2.128000e+09</td>\n",
       "      <td>15132475.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>5436.000000</td>\n",
       "      <td>60000.788000</td>\n",
       "      <td>13200.136000</td>\n",
       "      <td>948301.237700</td>\n",
       "      <td>31381.148000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>4.279712e+09</td>\n",
       "      <td>4.288214e+09</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>2.178828</td>\n",
       "      <td>1.176955</td>\n",
       "      <td>1.125113</td>\n",
       "      <td>1502.000000</td>\n",
       "      <td>1342.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>383482.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dur        spkts         dpkts         sbytes           dbytes  \\\n",
       "count  412.000000   412.000000    412.000000     412.000000       412.000000   \n",
       "mean     0.802402    17.798544     40.186893    4061.817961     43975.111650   \n",
       "std      3.955333    83.727403    537.250046   17026.318119    721052.362075   \n",
       "min      0.000000     1.000000      0.000000      46.000000         0.000000   \n",
       "25%      0.000008     2.000000      0.000000     114.000000         0.000000   \n",
       "50%      0.005863     6.000000      2.000000     564.000000       178.000000   \n",
       "75%      0.663749    12.000000     10.000000    1540.000000       830.000000   \n",
       "max     59.983105  1642.000000  10872.000000  185550.000000  14590859.000000   \n",
       "\n",
       "                 rate        sttl        dttl         sload            dload  \\\n",
       "count      412.000000  412.000000  412.000000  4.120000e+02       412.000000   \n",
       "mean     95889.921817  180.293689   91.041262  8.269606e+07    689131.999606   \n",
       "std     178648.222057  102.195241  114.941647  2.251225e+08   2412412.100645   \n",
       "min          0.000000    0.000000    0.000000  0.000000e+00         0.000000   \n",
       "25%         28.864416   62.000000    0.000000  1.258315e+04         0.000000   \n",
       "50%       2798.509862  254.000000   29.000000  6.278774e+05      1838.408874   \n",
       "75%     125000.000300  254.000000  252.000000  8.888889e+07     23318.995120   \n",
       "max    1000000.003000  255.000000  252.000000  2.128000e+09  15132475.000000   \n",
       "\n",
       "            sloss        dloss        sinpkt        dinpkt           sjit  \\\n",
       "count  412.000000   412.000000    412.000000    412.000000     412.000000   \n",
       "mean     3.201456    17.939320    388.406615     96.163556    6188.389576   \n",
       "std      7.535525   268.617491   4218.624716    875.950877   60571.784329   \n",
       "min      0.000000     0.000000      0.000000      0.000000       0.000000   \n",
       "25%      0.000000     0.000000      0.008000      0.000000       0.000000   \n",
       "50%      1.000000     0.000000      0.479206      0.009000      16.318723   \n",
       "75%      3.000000     2.000000     55.984806     55.786144    3026.191321   \n",
       "max     76.000000  5436.000000  60000.788000  13200.136000  948301.237700   \n",
       "\n",
       "               djit        swin         stcpb         dtcpb        dwin  \\\n",
       "count    412.000000  412.000000  4.120000e+02  4.120000e+02  412.000000   \n",
       "mean     462.261406  133.689320  1.097451e+09  9.519038e+08  125.643204   \n",
       "std     2437.142307  127.504518  1.402489e+09  1.278484e+09  127.641478   \n",
       "min        0.000000    0.000000  0.000000e+00  0.000000e+00    0.000000   \n",
       "25%        0.000000    0.000000  0.000000e+00  0.000000e+00    0.000000   \n",
       "50%        0.000000  255.000000  0.000000e+00  0.000000e+00    0.000000   \n",
       "75%      110.168092  255.000000  2.191444e+09  1.822121e+09  255.000000   \n",
       "max    31381.148000  255.000000  4.279712e+09  4.288214e+09  255.000000   \n",
       "\n",
       "           tcprtt      synack      ackdat        smean        dmean  \\\n",
       "count  412.000000  412.000000  412.000000   412.000000   412.000000   \n",
       "mean     0.056769    0.027610    0.029159   159.179612   109.565534   \n",
       "std      0.145878    0.071078    0.084931   233.105287   232.349898   \n",
       "min      0.000000    0.000000    0.000000    39.000000     0.000000   \n",
       "25%      0.000000    0.000000    0.000000    57.000000     0.000000   \n",
       "50%      0.000000    0.000000    0.000000    65.000000    44.000000   \n",
       "75%      0.103908    0.046738    0.046070   113.000000    81.000000   \n",
       "max      2.178828    1.176955    1.125113  1502.000000  1342.000000   \n",
       "\n",
       "       trans_depth  response_body_len  ct_srv_src  ct_state_ttl  ct_dst_ltm  \\\n",
       "count   412.000000         412.000000  412.000000    412.000000  412.000000   \n",
       "mean      0.084951        1343.293689    9.881068      1.381068    5.854369   \n",
       "std       0.279148       19574.748404   11.039541      1.151718    7.960557   \n",
       "min       0.000000           0.000000    1.000000      0.000000    1.000000   \n",
       "25%       0.000000           0.000000    2.000000      1.000000    1.000000   \n",
       "50%       0.000000           0.000000    6.000000      1.000000    3.000000   \n",
       "75%       0.000000           0.000000   12.000000      2.000000    6.000000   \n",
       "max       1.000000      383482.000000   53.000000      6.000000   45.000000   \n",
       "\n",
       "       ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  \\\n",
       "count        412.000000        412.000000      412.000000    412.000000   \n",
       "mean           4.813107          3.490291        7.611650      0.012136   \n",
       "std            7.908019          5.594784       11.341617      0.109626   \n",
       "min            1.000000          1.000000        1.000000      0.000000   \n",
       "25%            1.000000          1.000000        1.000000      0.000000   \n",
       "50%            1.000000          1.000000        3.000000      0.000000   \n",
       "75%            4.000000          3.000000        6.000000      0.000000   \n",
       "max           45.000000         28.000000       53.000000      1.000000   \n",
       "\n",
       "       ct_ftp_cmd  ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  \n",
       "count  412.000000        412.000000  412.000000  412.000000       412.000000  \n",
       "mean     0.012136          0.126214    6.359223    9.550971         0.007282  \n",
       "std      0.109626          0.611004    7.919122   11.108613         0.085124  \n",
       "min      0.000000          0.000000    1.000000    1.000000         0.000000  \n",
       "25%      0.000000          0.000000    2.000000    2.000000         0.000000  \n",
       "50%      0.000000          0.000000    3.000000    5.000000         0.000000  \n",
       "75%      0.000000          0.000000    7.000000   12.000000         0.000000  \n",
       "max      1.000000          9.000000   45.000000   53.000000         1.000000  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "dfcopy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:09 Clustering and Discretization\n",
    "Since we have a 190 features after 1 hot encoding, lets iterate through various pair wise classifications with the existing features to see what the best baseline classification we have\n",
    "to perform futher clustering techniques upon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.00000000e-06   2.00000000e+00   0.00000000e+00 ...,   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.10000000e-05   2.00000000e+00   0.00000000e+00 ...,   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.93578000e-01   1.00000000e+01   6.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " ..., \n",
      " [  1.03800000e-03   2.00000000e+00   2.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  3.41045900e+00   1.20000000e+01   1.00000000e+01 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.15200000e-03   2.00000000e+00   2.00000000e+00 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "Average accuracy =  91.5331010453 +- 4.16095571098\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'n_features_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-71b3ed4583ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Average accuracy = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"+-\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"n_features_: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'n_features_'"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print(X)\n",
    "\n",
    "cv = StratifiedKFold(y,n_folds=10)\n",
    "# n_estimators = The number of trees in the forest.\n",
    "clf = RandomForestClassifier(n_estimators=150,random_state=1)\n",
    "\n",
    "acc = cross_val_score(clf,X,y=y,cv=cv)\n",
    "\n",
    "print \"Average accuracy = \", acc.mean()*100, \"+-\", acc.std()*100\n",
    "print \"n_features_: \" + clf.n_features_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Adjust Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Models:\n",
    "        K-Means - PETER\n",
    "        MiniBatch - RANDY\n",
    "        DBSCAN - RAVI\n",
    "        GMM - DANIEL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "Reference:\n",
    "(a) Introduction to Data Mining (Authors: Pang-Ning Tan, Michael Steinbach, Vipin Kumar)\n",
    "\n",
    "Per ref (a) K-means clustering technique is a prototype-based, partitional clustering technique that attempts to find a users-specified number of clusters (K), which are represented by their centroids.\n",
    "    In our particular dataset, we will be specifing K = 2 to see how well the clustering algorithm can discover the desired labels of malcious vs normal packets.  We will then run the algorith with K = 10 which would be the number of attack categories that were identified manually by hand from the dataset. Although, in a non simulated real world environment, this number may not really be known and hence this particular cluster algorithm may not be very practical to try and differentiate packets into category types as generally you would not know apriori the total number of attack categories over a batch of packets.  That said, making the assumption that there are normal vs abnormal packets may be a reasonable thing to suppose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('length of data = ', 20582)\n",
      "2\n",
      "Estimated number of clusters: 2\n",
      "Homogeneity: 0.065\n",
      "V-measure: 0.066\n",
      "Adjusted Rand Index: 0.096\n",
      "Adjusted Mutual Information: 0.065\n",
      "Silhouette Coefficient: 0.653\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEKCAYAAAAGvn7fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/D/JfcgIP6pcqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcd19c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xce22f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "n_clusters = 2\n",
    "data = X\n",
    "model = KMeans(init='k-means++', n_clusters=n_clusters, n_init=1).fit(data)\n",
    "# model = AgglomerativeClustering(linkage='ward', n_clusters=n_clusters).fit(data)\n",
    "#model = DBSCAN(eps=0.15, min_samples=10).fit(data)\n",
    "s_size = len(data)/2\n",
    "print(\"length of data = \", s_size)\n",
    "labels_pred = model.labels_\n",
    "labels_true = y\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print( len(set(labels)))\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels_pred))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels_pred))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels_pred))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels_pred))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(data, labels_pred, sample_size=s_size))\n",
    "\n",
    "# Got memory error with silhouette/ reduced sample size in half\n",
    "silhouette_per_sample = metrics.silhouette_samples(data,labels_pred)\n",
    "\n",
    "# get the middle of each cluster\n",
    "centroids = []\n",
    "for lab in range(0,n_clusters_):\n",
    "\n",
    "    centroids.append( [np.mean(data[np.where(labels_pred==lab),0]), np.mean(data[np.where(labels_pred==lab),1])] )\n",
    "centroids = np.array(centroids)\n",
    "\n",
    "plt.figure()\n",
    "#plt.scatter(data[:, 0], data[:, 1], c=labels,           \n",
    "#                    cmap=plt.cm.spectral, s=5, linewidths=0)\n",
    "\n",
    "if len(centroids)>0:\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='+', s=200, linewidths=3, color='k', zorder=10)  # plot the centroids\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(data[:, 0], data[:, 1], c=silhouette_per_sample,\n",
    "                cmap=plt.cm.gray, s=5, linewidths=0)\n",
    "plt.title('Clusters with silhouette coefficient coloring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained variance ratio (first 3 components): [ 80.59365547  18.59309631   0.81310102]\n",
      "0.999998528038\n"
     ]
    }
   ],
   "source": [
    "#curious if we run do feature reduction with PCA and run cluster algorith on reduced features if \n",
    "# the metrics like homogeneity improve\n",
    "pca = PCA(n_components=3)\n",
    "x_pca = pca.fit(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first 3 components): %s' % (100 * pca.explained_variance_ratio_) )\n",
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_digits(num categories): 2, \t n_samples 42, \t n_features 42\n",
      "_______________________________________________________________________________\n",
      "init    time  inertia    homo   compl  v-meas     ARI AMI  silhouette\n",
      "k-means++   0.43s    52908653520319819022336   1.000   1.000   1.000   1.000   1.000    0.602\n",
      "   random   0.33s    52908655521244209217536   1.000   1.000   1.000   1.000   1.000    0.590\n",
      "PCA-based   0.38s    52908653520319819022336   1.000   1.000   1.000   1.000   1.000    0.639\n",
      "_______________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# ref: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py\n",
    "s_size = 42\n",
    "sample_size = s_size\n",
    "n_samples = s_size\n",
    "n_digits = n_clusters  \n",
    "n_features = len(dfcopy.columns)\n",
    "\n",
    "\n",
    "print(\"n_digits(num categories): %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))\n",
    "\n",
    "\n",
    "print(79 * '_')\n",
    "print('% 9s' % 'init'\n",
    "      '    time  inertia    homo   compl  v-meas     ARI AMI  silhouette')\n",
    "\n",
    "\n",
    "def bench_k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "   \n",
    "bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=1),\n",
    "              name=\"k-means++\", data=data)\n",
    "\n",
    "bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=1),\n",
    "              name=\"random\", data=data)\n",
    "\n",
    "# in this case the seeding of the centers is deterministic, hence we run the\n",
    "# kmeans algorithm only once with n_init=1\n",
    "pca = PCA(n_components=n_digits).fit(data)\n",
    "bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n",
    "              name=\"PCA-based\",\n",
    "              data=data)\n",
    "print(79 * '_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-9a94a6ef2ef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mx_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduced_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0my_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduced_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Obtain labels for each point in mesh. Use last trained model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reduced_data = PCA(n_components=2).fit_transform(data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=1)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the packets dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ref:\n",
    "(a)http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html\n",
    "    score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Can we visualize a data set with this many features?\n",
    "    Can we visualize on the attack categories?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the Ramifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resuse from Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Be critical of your performance and tell the reader how you current model might be usable by other parties. Did you achieve your goals? If not, can you reign in the utility of your modeling?*\n",
    "- *How useful is your model for interested parties (i.e., the companies or organizations that might want to use it)?*\n",
    "- *How would your deploy your model for interested parties?*\n",
    "- *What other data should be collected?*\n",
    "- *How often would the model need to be updated, etc.?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
